{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":24286,"databundleVersionId":1878097,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 📦 Imports\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import normalize\nfrom transformers import AutoTokenizer, AutoModel\nfrom timm import create_model\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport faiss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:03:37.425685Z","iopub.execute_input":"2025-05-14T05:03:37.426035Z","iopub.status.idle":"2025-05-14T05:03:55.226392Z","shell.execute_reply.started":"2025-05-14T05:03:37.426007Z","shell.execute_reply":"2025-05-14T05:03:55.225436Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install faiss-cpu --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:03:04.609214Z","iopub.execute_input":"2025-05-14T05:03:04.609528Z","iopub.status.idle":"2025-05-14T05:03:08.086460Z","shell.execute_reply.started":"2025-05-14T05:03:04.609501Z","shell.execute_reply":"2025-05-14T05:03:08.085296Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ⚙️ Config\nclass CFG:\n    image_model = \"eca_nfnet_l1\"\n    text_model = \"xlm-roberta-base\"\n    image_size = 256\n    batch_size = 64\n    num_workers = 2\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:05.467457Z","iopub.execute_input":"2025-05-14T05:04:05.467803Z","iopub.status.idle":"2025-05-14T05:04:05.472415Z","shell.execute_reply.started":"2025-05-14T05:04:05.467775Z","shell.execute_reply":"2025-05-14T05:04:05.471496Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 📄 Load Data\ndf = pd.read_csv(\"/kaggle/input/shopee-product-matching/train.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:09.097036Z","iopub.execute_input":"2025-05-14T05:04:09.097348Z","iopub.status.idle":"2025-05-14T05:04:09.288153Z","shell.execute_reply.started":"2025-05-14T05:04:09.097324Z","shell.execute_reply":"2025-05-14T05:04:09.287240Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#STEP-1\n# 🖼️ Image Transform\ntransform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(),\n    ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:13.224269Z","iopub.execute_input":"2025-05-14T05:04:13.225069Z","iopub.status.idle":"2025-05-14T05:04:13.233390Z","shell.execute_reply.started":"2025-05-14T05:04:13.225041Z","shell.execute_reply":"2025-05-14T05:04:13.232570Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#STEP-2\ntransform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.ShiftScaleRotate(p=0.3),\n    A.Normalize(),\n    ToTensorV2()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:17.023988Z","iopub.execute_input":"2025-05-14T05:04:17.024305Z","iopub.status.idle":"2025-05-14T05:04:17.035346Z","shell.execute_reply.started":"2025-05-14T05:04:17.024274Z","shell.execute_reply":"2025-05-14T05:04:17.034368Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:58: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 📦 Dataset\nclass ShopeeDataset(Dataset):\n    def __init__(self, df, mode=\"image\"):\n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.tokenizer = AutoTokenizer.from_pretrained(CFG.text_model)\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        if self.mode == \"image\":\n            image = cv2.imread(f\"/kaggle/input/shopee-product-matching/train_images/{row.image}\")\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = transform(image=image)[\"image\"]\n            return image\n        else:\n            text = row.title\n            inputs = self.tokenizer(text, padding=\"max_length\", truncation=True,\n                                    return_tensors=\"pt\", max_length=64)\n            return {k: v.squeeze(0) for k, v in inputs.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:27.625564Z","iopub.execute_input":"2025-05-14T05:04:27.626532Z","iopub.status.idle":"2025-05-14T05:04:27.633487Z","shell.execute_reply.started":"2025-05-14T05:04:27.626503Z","shell.execute_reply":"2025-05-14T05:04:27.632424Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Image Model\nclass ImageEmbeddingModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = create_model(CFG.image_model, pretrained=True, num_classes=0)\n        self.bn = torch.nn.BatchNorm1d(self.backbone.num_features)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.bn(x)\n        x = torch.nn.functional.normalize(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:32.725208Z","iopub.execute_input":"2025-05-14T05:04:32.725508Z","iopub.status.idle":"2025-05-14T05:04:32.730728Z","shell.execute_reply.started":"2025-05-14T05:04:32.725485Z","shell.execute_reply":"2025-05-14T05:04:32.729787Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Text Model\nclass TextEmbeddingModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(CFG.text_model)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        x = out.last_hidden_state[:, 0, :]\n        x = torch.nn.functional.normalize(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:36.067372Z","iopub.execute_input":"2025-05-14T05:04:36.067693Z","iopub.status.idle":"2025-05-14T05:04:36.073147Z","shell.execute_reply.started":"2025-05-14T05:04:36.067664Z","shell.execute_reply":"2025-05-14T05:04:36.072076Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Embedding Extraction\ndef get_embeddings(model, loader, mode=\"image\"):\n    model.eval()\n    embeds = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            if mode == \"image\":\n                batch = batch.to(CFG.device)\n                emb = model(batch)\n            else:\n                batch = {k: v.to(CFG.device) for k, v in batch.items()}\n                emb = model(**batch)\n            embeds.append(emb.cpu())\n    return torch.cat(embeds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:40.399326Z","iopub.execute_input":"2025-05-14T05:04:40.399963Z","iopub.status.idle":"2025-05-14T05:04:40.405388Z","shell.execute_reply.started":"2025-05-14T05:04:40.399919Z","shell.execute_reply":"2025-05-14T05:04:40.404564Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#  FAISS Matching\ndef get_matches(embeddings, threshold=0.5):\n    embeddings = normalize(embeddings.numpy())\n    index = faiss.IndexFlatIP(embeddings.shape[1])\n    index.add(embeddings)\n    D, I = index.search(embeddings, 50)\n    \n    matches = []\n    for i in range(len(I)):\n        ids = I[i][D[i] > threshold]\n        matches.append(df.iloc[ids].posting_id.values)\n    return matches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:43.515132Z","iopub.execute_input":"2025-05-14T05:04:43.515419Z","iopub.status.idle":"2025-05-14T05:04:43.521040Z","shell.execute_reply.started":"2025-05-14T05:04:43.515400Z","shell.execute_reply":"2025-05-14T05:04:43.519971Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#  Evaluation\ndef f1_score(df):\n    gt = df.groupby(\"label_group\")[\"posting_id\"].apply(set).to_dict()\n    df[\"true\"] = df[\"label_group\"].map(gt)\n    df[\"pred\"] = df[\"matches\"].apply(lambda x: set(x.split()))\n    \n    def f1(row):\n        inter = len(row.true & row.pred)\n        if not row.pred: return 0\n        prec = inter / len(row.pred)\n        rec = inter / len(row.true)\n        return 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0\n\n    return df.apply(f1, axis=1).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:04:48.753668Z","iopub.execute_input":"2025-05-14T05:04:48.754080Z","iopub.status.idle":"2025-05-14T05:04:48.762167Z","shell.execute_reply.started":"2025-05-14T05:04:48.754048Z","shell.execute_reply":"2025-05-14T05:04:48.761059Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# ▶️ Main Execution\ndef main():\n    # Image embeddings\n    image_ds = ShopeeDataset(df, mode=\"image\")\n    image_loader = DataLoader(image_ds, batch_size=CFG.batch_size, num_workers=CFG.num_workers)\n    image_model = ImageEmbeddingModel().to(CFG.device)\n    image_embs = get_embeddings(image_model, image_loader, mode=\"image\")\n\n    # Text embeddings\n    text_ds = ShopeeDataset(df, mode=\"text\")\n    text_loader = DataLoader(text_ds, batch_size=CFG.batch_size, num_workers=CFG.num_workers)\n    text_model = TextEmbeddingModel().to(CFG.device)\n    text_embs = get_embeddings(text_model, text_loader, mode=\"text\")\n\n    # Combine and normalize\n    combined = normalize(np.hstack([image_embs, text_embs]))\n\n    # Find matches\n    match_ids = get_matches(torch.tensor(combined), threshold=0.5)\n    df[\"matches\"] = [\" \".join(m) for m in match_ids]\n\n    # Score\n    score = f1_score(df)\n    print(f\"Validation F1 Score: {score:.5f}\")\n\nmain()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ▶️ Main Execution\ndef main():\n    # Image embeddings\n    image_ds = ShopeeDataset(df, mode=\"image\")\n    image_loader = DataLoader(image_ds, batch_size=CFG.batch_size, num_workers=CFG.num_workers)\n    image_model = ImageEmbeddingModel().to(CFG.device)\n    image_embs = get_embeddings(image_model, image_loader, mode=\"image\").numpy()\n\n    # Text embeddings\n    text_ds = ShopeeDataset(df, mode=\"text\")\n    text_loader = DataLoader(text_ds, batch_size=CFG.batch_size, num_workers=CFG.num_workers)\n    text_model = TextEmbeddingModel().to(CFG.device)\n    text_embs = get_embeddings(text_model, text_loader, mode=\"text\").numpy()\n\n    # Normalize embeddings separately\n    image_embs = normalize(image_embs)\n    text_embs = normalize(text_embs)\n\n    # Test different embedding weights and thresholds\n    best_score, best_thresh, best_weight = 0, 0, 0\n\n    weights = [0.2, 0.4, 0.5, 0.6, 0.8]\n    thresholds = np.arange(0.3, 0.8, 0.05)\n\n    for w in weights:\n        #combined = normalize(w * image_embs + (1 - w) * text_embs)\n        combined = normalize(np.hstack([image_embs * w, text_embs * (1 - w)]))\n\n        for thresh in thresholds:\n            matches = get_matches(torch.tensor(combined), threshold=thresh)\n            df[\"matches\"] = [\" \".join(m) for m in matches]\n            score = f1_score(df)\n\n            print(f\"Weight: Img {w:.2f}/Text {1-w:.2f} | Threshold: {thresh:.2f} | F1 Score: {score:.5f}\")\n\n            if score > best_score:\n                best_score = score\n                best_thresh = thresh\n                best_weight = w\n\n    print(\"\\n🏆 Best Results:\")\n    print(f\"Best Embedding Weight: Image {best_weight:.2f} | Text {1 - best_weight:.2f}\")\n    print(f\"Best Threshold: {best_thresh:.2f}\")\n    print(f\"Best Validation F1 Score: {best_score:.5f}\")\n\nmain()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T00:24:50.688328Z","iopub.execute_input":"2025-05-07T00:24:50.689170Z","execution_failed":"2025-05-07T02:47:18.303Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 536/536 [04:59<00:00,  1.79it/s]\n100%|██████████| 536/536 [00:56<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Weight: Img 0.20/Text 0.80 | Threshold: 0.30 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.35 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.40 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.45 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.50 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.55 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.60 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.65 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.70 | F1 Score: 0.14911\nWeight: Img 0.20/Text 0.80 | Threshold: 0.75 | F1 Score: 0.14912\nWeight: Img 0.40/Text 0.60 | Threshold: 0.30 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.35 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.40 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.45 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.50 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.55 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.60 | F1 Score: 0.14878\nWeight: Img 0.40/Text 0.60 | Threshold: 0.65 | F1 Score: 0.14879\nWeight: Img 0.40/Text 0.60 | Threshold: 0.70 | F1 Score: 0.14880\nWeight: Img 0.40/Text 0.60 | Threshold: 0.75 | F1 Score: 0.14954\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ▶️ Main Execution (Improved)\ndef main():\n    print(\"🔄 Extracting image embeddings...\")\n    image_ds = ShopeeDataset(df, mode=\"image\")\n    image_loader = DataLoader(image_ds, batch_size=CFG.batch_size, num_workers=CFG.num_workers)\n    image_model = ImageEmbeddingModel().to(CFG.device)\n    image_embs = get_embeddings(image_model, image_loader, mode=\"image\")\n    \n    print(\"🔄 Extracting text embeddings...\")\n    text_ds = ShopeeDataset(df, mode=\"text\")\n    text_loader = DataLoader(text_ds, batch_size=CFG.batch_size, num_workers=CFG.num_workers)\n    text_model = TextEmbeddingModel().to(CFG.device)\n    text_embs = get_embeddings(text_model, text_loader, mode=\"text\")\n\n    # ✅ Normalize individually before concatenating\n    print(\"📐 Normalizing embeddings...\")\n    image_embs = normalize(image_embs)\n    text_embs = normalize(text_embs)\n    combined = normalize(np.hstack([image_embs, text_embs]))\n\n    # ✅ Use a lower threshold to catch more matches\n    print(\"🔍 Running FAISS similarity search...\")\n    match_ids = get_matches(torch.tensor(combined), threshold=0.3)\n\n    # Format match output as strings\n    df[\"matches\"] = [\" \".join(map(str, m)) for m in match_ids]\n\n    # Evaluate\n    print(\"📊 Calculating F1 score...\")\n    score = f1_score(df)\n    print(f\"\\n✅ Validation F1 Score: {score:.5f}\")\n\nmain()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T00:55:46.250494Z","iopub.execute_input":"2025-05-03T00:55:46.250988Z","iopub.status.idle":"2025-05-03T01:03:00.893065Z","shell.execute_reply.started":"2025-05-03T00:55:46.250963Z","shell.execute_reply":"2025-05-03T01:03:00.892081Z"}},"outputs":[{"name":"stdout","text":"🔄 Extracting image embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639df87ed9df4951990be9e55fa4c738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5b14f392834de2991e95211b8c45f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66a95cccc80452689e270d0a9ce495c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c9cae265fd487fb13b22756bcdce1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0c4686eaad54717afdc0d1c0fe0a568"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 536/536 [05:08<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"🔄 Extracting text embeddings...\n","output_type":"stream"},{"name":"stderr","text":"2025-05-03 01:01:04.815440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746234065.004445      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746234065.063236      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e56496154034bb3b73754342609e3fd"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 536/536 [00:56<00:00,  9.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"📐 Normalizing embeddings...\n🔍 Running FAISS similarity search...\n📊 Calculating F1 score...\n\n✅ Validation F1 Score: 0.14872\n","output_type":"stream"}],"execution_count":13}]}